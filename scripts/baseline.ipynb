{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"ijvIxku9LS0g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhnvipLOS5Or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702185251285,"user_tz":300,"elapsed":4,"user":{"displayName":"Jason Chen","userId":"16265356244672233075"}},"outputId":"2ddccbba-8e9c-4153-d20c-5938c04a1bc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n"]}],"source":["from google.colab import drive\n","drive.flush_and_unmount()"]},{"cell_type":"code","source":["#Connect to GDrive\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","#Emma's directory:\n","#%cd gdrive/MyDrive/6.8610\\ Research\\ Project/experiments\n","#David's directory:\n","#%cd gdrive/MyDrive/experiments\n","#Jason's directory:\n","%cd gdrive/MyDrive/6.8610\\ Research\\ Project/experiments\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pIWUn2XLS_NA","executionInfo":{"status":"ok","timestamp":1702185271761,"user_tz":300,"elapsed":17566,"user":{"displayName":"Jason Chen","userId":"16265356244672233075"}},"outputId":"93c6d10d-0164-4b02-c825-7b5a36a40e82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/.shortcut-targets-by-id/1MAHUubRH-c0eGy0WdGID3SXRXAsmlUfq/6.8610 Research Project/experiments\n"]}]},{"cell_type":"code","source":["import json\n","import io\n","from google.colab import files\n","import torch\n","import pandas as pd\n","!pip install sacrebleu\n","import sacrebleu\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, LogitsProcessorList\n","from extended_watermark_processor import WatermarkLogitsProcessor, WatermarkDetector, WatermarkBase"],"metadata":{"id":"D1iGBkPHTWSZ","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1702157290388,"user_tz":300,"elapsed":109,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"59342e68-0271-42e6-bc54-a06c0b45966a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-44b49b851723>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sacrebleu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msacrebleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n","\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"]}]},{"cell_type":"markdown","source":["# Manual Imports"],"metadata":{"id":"4QKMTd7Lz8Pk"}},{"cell_type":"code","source":["######### HOMOGLYPHS.py ##########\n","\"\"\"Updated version of core.py from\n","https://github.com/yamatt/homoglyphs/tree/main/homoglyphs_fork\n","for modern python3\n","\"\"\"\n","\n","from collections import defaultdict\n","import json\n","from itertools import product\n","import os\n","import unicodedata\n","\n","# Actions if char not in alphabet\n","STRATEGY_LOAD = 1  # load category for this char\n","STRATEGY_IGNORE = 2  # add char to result\n","STRATEGY_REMOVE = 3  # remove char from result\n","\n","ASCII_RANGE = range(128)\n","\n","\n","#CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n","#DATA_LOCATION = os.path.join(CURRENT_DIR, \"homoglyph_data\")\n","DATA_LOCATION = 'gdrive/MyDrive/experiments'\n","\n","class Categories:\n","    \"\"\"\n","    Work with aliases from ISO 15924.\n","    https://en.wikipedia.org/wiki/ISO_15924#List_of_codes\n","    \"\"\"\n","\n","    #fpath = os.path.join(DATA_LOCATION, \"categories.json\")\n","    fpath = 'gdrive/MyDrive/experiments/categories.json'\n","    @classmethod\n","    def _get_ranges(cls, categories):\n","        \"\"\"\n","        :return: iter: (start code, end code)\n","        :rtype: list\n","        \"\"\"\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","\n","        for category in categories:\n","            if category not in data[\"aliases\"]:\n","                raise ValueError(\"Invalid category: {}\".format(category))\n","\n","        for point in data[\"points\"]:\n","            if point[2] in categories:\n","                yield point[:2]\n","\n","    @classmethod\n","    def get_alphabet(cls, categories):\n","        \"\"\"\n","        :return: set of chars in alphabet by categories list\n","        :rtype: set\n","        \"\"\"\n","        alphabet = set()\n","        for start, end in cls._get_ranges(categories):\n","            chars = (chr(code) for code in range(start, end + 1))\n","            alphabet.update(chars)\n","        return alphabet\n","\n","    @classmethod\n","    def detect(cls, char):\n","        \"\"\"\n","        :return: category\n","        :rtype: str\n","        \"\"\"\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","\n","        # try detect category by unicodedata\n","        try:\n","            category = unicodedata.name(char).split()[0]\n","        except (TypeError, ValueError):\n","            # In Python2 unicodedata.name raise error for non-unicode chars\n","            # Python3 raise ValueError for non-unicode characters\n","            pass\n","        else:\n","            if category in data[\"aliases\"]:\n","                return category\n","\n","        # try detect category by ranges from JSON file.\n","        code = ord(char)\n","        for point in data[\"points\"]:\n","            if point[0] <= code <= point[1]:\n","                return point[2]\n","\n","    @classmethod\n","    def get_all(cls):\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        return set(data[\"aliases\"])\n","\n","\n","class Languages:\n","    #fpath = os.path.join(DATA_LOCATION, \"languages.json\")\n","    fpath = 'gdrive/MyDrive/experiments/homoglyph_data/languages.json'\n","    @classmethod\n","    def get_alphabet(cls, languages):\n","        \"\"\"\n","        :return: set of chars in alphabet by languages list\n","        :rtype: set\n","        \"\"\"\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        alphabet = set()\n","            if lang not in data:\n","                raise ValueError(\"Invalid language code: {}\".format(lang))\n","            alphabet.update(data[lang])\n","        return alphabet\n","\n","    @classmethod\n","    def detect(cls, char):\n","        \"\"\"\n","        :return: set of languages which alphabet contains passed char.\n","        :rtype: set\n","        \"\"\"\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        languages = set()\n","        for lang, alphabet in data.items():\n","            if char in alphabet:\n","                languages.add(lang)\n","        return languages\n","\n","    @classmethod\n","    def get_all(cls):\n","        with open(cls.fpath, encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        return set(data.keys())\n","\n","\n","class Homoglyphs:\n","    def __init__(\n","        self,\n","        categories=None,\n","        languages=None,\n","        alphabet=None,\n","        strategy=STRATEGY_IGNORE,\n","        ascii_strategy=STRATEGY_IGNORE,\n","        ascii_range=ASCII_RANGE,\n","    ):\n","        # strategies\n","        if strategy not in (STRATEGY_LOAD, STRATEGY_IGNORE, STRATEGY_REMOVE):\n","            raise ValueError(\"Invalid strategy\")\n","        self.strategy = strategy\n","        self.ascii_strategy = ascii_strategy\n","        self.ascii_range = ascii_range\n","\n","        # Homoglyphs must be initialized by any alphabet for correct work\n","        if not categories and not languages and not alphabet:\n","            categories = (\"LATIN\", \"COMMON\")\n","\n","        # cats and langs\n","        self.categories = set(categories or [])\n","        self.languages = set(languages or [])\n","\n","        # alphabet\n","        self.alphabet = set(alphabet or [])\n","        if self.categories:\n","            alphabet = Categories.get_alphabet(self.categories)\n","            self.alphabet.update(alphabet)\n","        if self.languages:\n","            alphabet = Languages.get_alphabet(self.languages)\n","            self.alphabet.update(alphabet)\n","        self.table = self.get_table(self.alphabet)\n","\n","    @staticmethod\n","    def get_table(alphabet):\n","        table = defaultdict(set)\n","        #with open(os.path.join(DATA_LOCATION, \"confusables_sept2022.json\")) as f:\n","        with open('gdrive/MyDrive/experiments/confusables_sept2022.json') as f:\n","            data = json.load(f)\n","        for char in alphabet:\n","            if char in data:\n","                for homoglyph in data[char]:\n","                    if homoglyph in alphabet:\n","                        table[char].add(homoglyph)\n","        return table\n","\n","    @staticmethod\n","    def get_restricted_table(source_alphabet, target_alphabet):\n","        table = defaultdict(set)\n","        #with open(os.path.join(DATA_LOCATION, \"confusables_sept2022.json\")) as f:\n","        with open('gdrive/MyDrive/experiments/confusables_sept2022') as f:\n","            data = json.load(f)\n","        for char in source_alphabet:\n","            if char in data:\n","                for homoglyph in data[char]:\n","                    if homoglyph in target_alphabet:\n","                        table[char].add(homoglyph)\n","        return table\n","\n","    @staticmethod\n","    def uniq_and_sort(data):\n","        result = list(set(data))\n","        result.sort(key=lambda x: (-len(x), x))\n","        return result\n","\n","    def _update_alphabet(self, char):\n","        # try detect languages\n","        langs = Languages.detect(char)\n","        if langs:\n","            self.languages.update(langs)\n","            alphabet = Languages.get_alphabet(langs)\n","            self.alphabet.update(alphabet)\n","        else:\n","            # try detect categories\n","            category = Categories.detect(char)\n","            if category is None:\n","                return False\n","            self.categories.add(category)\n","            alphabet = Categories.get_alphabet([category])\n","            self.alphabet.update(alphabet)\n","        # update table for new alphabet\n","        self.table = self.get_table(self.alphabet)\n","        return True\n","\n","    def _get_char_variants(self, char):\n","        if char not in self.alphabet:\n","            if self.strategy == STRATEGY_LOAD:\n","                if not self._update_alphabet(char):\n","                    return []\n","            elif self.strategy == STRATEGY_IGNORE:\n","                return [char]\n","            elif self.strategy == STRATEGY_REMOVE:\n","                return []\n","\n","        # find alternative chars for current char\n","        alt_chars = self.table.get(char, set())\n","        if alt_chars:\n","            # find alternative chars for alternative chars for current char\n","            alt_chars2 = [self.table.get(alt_char, set()) for alt_char in alt_chars]\n","            # combine all alternatives\n","            alt_chars.update(*alt_chars2)\n","        # add current char to alternatives\n","        alt_chars.add(char)\n","\n","        # uniq, sort and return\n","        return self.uniq_and_sort(alt_chars)\n","\n","    def _get_combinations(self, text, ascii=False):\n","        variations = []\n","        for char in text:\n","            alt_chars = self._get_char_variants(char)\n","\n","            if ascii:\n","                alt_chars = [char for char in alt_chars if ord(char) in self.ascii_range]\n","                if not alt_chars and self.ascii_strategy == STRATEGY_IGNORE:\n","                    return\n","\n","            if alt_chars:\n","                variations.append(alt_chars)\n","        if variations:\n","            for variant in product(*variations):\n","                yield \"\".join(variant)\n","\n","    def get_combinations(self, text):\n","        return list(self._get_combinations(text))\n","\n","    def _to_ascii(self, text):\n","        for variant in self._get_combinations(text, ascii=True):\n","            if max(map(ord, variant)) in self.ascii_range:\n","                yield variant\n","\n","    def to_ascii(self, text):\n","        return self.uniq_and_sort(self._to_ascii(text))\n"],"metadata":{"id":"5KFGuO7_z9zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######### ALTERNATIVE PRF SCHEMES.py ##########\n","\"\"\"Implement other PRF functions (These all vary only how they generate a single hash from the tokens in the context).\n","\n","Can be hooked into existing WatermarkLogitsProcessor as modified base class WatermarkBase, see implementation in\n","extended_watermark_processor.py\n","\"\"\"\n","\n","# coding=utf-8\n","# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n","# available at https://arxiv.org/abs/2301.10226\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","import torch\n","from itertools import combinations\n","from functools import cache\n","\n","# Key properties of a hashing scheme\n","props = {\n","    \"prf_type\": str,  # string name of the underlying PRF mapping multiple token ids to a random seed\n","    \"context_width\": int,  # this is h in the paper, how many previous tokens should be considered for each PRF\n","    \"self_salt\": bool,  # Use the rules laid in robust-watermarking to use the token itself to seed and possibly reject its own list\n","    \"hash_key\": int,  # integer, large prime, used to move seed away from low-entrop bit sequences in PRF chosen above\n","}\n","\n","\n","def seeding_scheme_lookup(seeding_scheme: str):\n","    if not isinstance(seeding_scheme, str):\n","        raise ValueError(\"Seeding scheme should be a string summarizing the procedure.\")\n","    if seeding_scheme == \"simple_1\" or seeding_scheme == \"lefthash\":\n","        # Default, simple bigram hash  # alias for ff-additive_prf-1-False-15485863\n","        prf_type = \"additive_prf\"\n","        context_width = 1\n","        self_salt = False\n","        hash_key = 15485863\n","    elif seeding_scheme == \"algorithm-3\" or seeding_scheme == \"selfhash\":\n","        prf_type = \"anchored_minhash_prf\"\n","        context_width = 4\n","        self_salt = True\n","        hash_key = 15485863\n","    elif seeding_scheme == \"minhash\":\n","        prf_type = \"minhash_prf\"\n","        context_width = 4\n","        self_salt = False\n","        hash_key = 15485863\n","    elif seeding_scheme == \"skipgram\":\n","        prf_type = \"skipgram_prf\"\n","        context_width = 5\n","        self_salt = False\n","        hash_key = 15485863\n","    elif seeding_scheme.startswith(\"ff\"):  # freeform seeding scheme API - only use for experimenting\n","        # expects strings of the form ff-additive_prf-4-True-hash or ff-additive_prf-5-True (hash key is optional)\n","        split_scheme = seeding_scheme.split(\"-\")\n","        prf_type = str(split_scheme[1])\n","        context_width = int(split_scheme[2])\n","        self_salt = split_scheme[3] == \"True\"\n","        if len(split_scheme) == 5:\n","            hash_key = int(split_scheme[4])\n","        else:\n","            hash_key = 15485863\n","    else:\n","        raise ValueError(f\"Invalid seeding scheme name {seeding_scheme} given. Try  'simple_1'?\")\n","\n","    assert prf_type in prf_lookup.keys()\n","    return prf_type, context_width, self_salt, hash_key\n","\n","\n","def multiplicative_prf(input_ids: torch.LongTensor, salt_key: int) -> int:\n","    return salt_key * input_ids.prod().item()\n","\n","\n","def additive_prf(input_ids: torch.LongTensor, salt_key: int) -> int:\n","    return salt_key * input_ids.sum().item()\n","\n","\n","def minfunc_prf(input_ids: torch.LongTensor, salt_key: int) -> int:\n","    # not a great idea for non-random input ids as in text\n","    return salt_key * input_ids.min().item()\n","\n","\n","def simple_skip_prf(input_ids: torch.LongTensor, salt_key: int, k=2) -> int:\n","    # k is the skip distance\n","    return hashint(salt_key * input_ids[::k]).prod().item()\n","\n","\n","def skipgram_prf(input_ids: torch.LongTensor, salt_key: int) -> int:\n","    # maximum distance skipgram within context\n","    return hashint(salt_key * input_ids[0]).item()\n","\n","\n","def anchored_skipgram_prf(input_ids: torch.LongTensor, salt_key: int, anchor: int = -1) -> int:\n","    # maximum distance skipgram within context\n","    return (hashint(salt_key * input_ids[0]) * hashint(salt_key * input_ids[anchor])).item()\n","\n","\n","def minhash_prf(input_ids: torch.LongTensor, salt_key: int) -> int:\n","    # slightly less not the greatest idea for non-random input ids as in text\n","    return hashint(salt_key * input_ids).min().item()\n","\n","\n","def anchored_minhash_prf(input_ids: torch.LongTensor, salt_key: int, anchor: int = -1) -> int:\n","    # Anchor to one key to produce a min over pairs again\n","    return (salt_key * hashint(input_ids) * hashint(input_ids[anchor])).min().item()\n","\n","\n","def minskipgram_prf(input_ids: torch.LongTensor, salt_key: int, k: int = 2) -> int:\n","    # min over all skipgrams in context, k=2 is all pairs\n","    skipgrams = torch.as_tensor(list(combinations(hashint(salt_key * input_ids), 2)))\n","    return skipgrams.prod(dim=1).min().item()\n","\n","\n","def noncomm_prf(input_ids: torch.LongTensor, salt_key: int, k: int = 2) -> int:\n","    key = torch.as_tensor(salt_key, dtype=torch.long)\n","    for entry in input_ids:\n","        key *= hashint(key * entry)\n","        key %= 2**32\n","    return key.item()\n","\n","\n","def position_prf(input_ids: torch.LongTensor, salt_key: int, k: int = 2) -> int:\n","    return (salt_key * input_ids * torch.arange(1, len(input_ids) + 1, device=input_ids.device)).sum().item()\n","\n","\n","prf_lookup = {\n","    \"multiplicative_prf\": multiplicative_prf,\n","    \"additive_prf\": additive_prf,\n","    \"minfunc_prf\": minfunc_prf,\n","    \"simple_skip_prf\": simple_skip_prf,\n","    \"skipgram_prf\": skipgram_prf,\n","    \"anchored_skipgram_prf\": anchored_skipgram_prf,\n","    \"minhash_prf\": minhash_prf,\n","    \"anchored_minhash_prf\": anchored_minhash_prf,\n","    \"minskipgram_prf\": minskipgram_prf,\n","    \"noncomm_prf\": noncomm_prf,\n","    \"position_prf\": position_prf,\n","}\n","\n","# Generate a global permute table once at startup\n","rng = torch.Generator(device=torch.device(\"cpu\"))\n","rng.manual_seed(2971215073)  # fib47 is prime\n","table_size = 1_000_003\n","fixed_table = torch.randperm(1_000_003, device=torch.device(\"cpu\"), generator=rng)  # actually faster than I thought\n","\n","\n","def hashint(integer_tensor: torch.LongTensor) -> torch.LongTensor:\n","    \"\"\"Sane version, in the end we only need a small permutation table.\"\"\"\n","    return fixed_table[integer_tensor.cpu() % table_size] + 1  # minor cheat here, this function always return CPU values\n","\n","\n","def _hashint_avalanche_tensor(integer_tensor: torch.LongTensor):\n","    \"\"\"http://burtleburtle.net/bob/hash/integer.html, ported into pytorch, runs on tensors. Apparently a decent avalanche.\"\"\"\n","    i = integer_tensor.to(torch.int32).clone()  # or torch.int16?\n","    i -= i << 6\n","    i ^= i >> 17\n","    i -= i << 9\n","    i ^= i << 4\n","    i -= i << 3\n","    i ^= i << 10\n","    i ^= i >> 15\n","    return i.to(torch.long)\n","\n","\n","@cache\n","def _hashint_avalanche_int(integer: int):\n","    \"\"\"http://burtleburtle.net/bob/hash/integer.html, runs in base python, caches based on access.\n","    Does this make sense for signed 64bit ints?\"\"\"\n","    i = integer % (2**32)\n","    i -= i << 6\n","    i ^= i >> 17\n","    i -= i << 9\n","    i ^= i << 4\n","    i -= i << 3\n","    i ^= i << 10\n","    i ^= i >> 15\n","    return i\n"],"metadata":{"id":"9RDpRmdzBTy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######### NORMALIZERS.py ##########\n","\"\"\" Text-based normalizers, used to mitigate simple attacks against watermarking.\n","\n","This implementation is unlikely to be a complete list of all possible exploits within the unicode standard,\n","it represents our best effort at the time of writing.\n","\n","These normalizers can be used as stand-alone normalizers. They could be made to conform to HF tokenizers standard, but that would\n","require messing with the limited rust interface of tokenizers.NormalizedString\n","\"\"\"\n","from collections import defaultdict\n","from functools import cache\n","\n","import re\n","import unicodedata\n","# import homoglyphs as hg\n","\n","\n","def normalization_strategy_lookup(strategy_name: str) -> object:\n","    if strategy_name == \"unicode\":\n","        return UnicodeSanitizer()\n","    elif strategy_name == \"homoglyphs\":\n","        return HomoglyphCanonizer()\n","    elif strategy_name == \"truecase\":\n","        return TrueCaser()\n","\n","\n","class HomoglyphCanonizer:\n","    \"\"\"Attempts to detect homoglyph attacks and find a consistent canon.\n","\n","    This function does so on a per-ISO-category level. Language-level would also be possible (see commented code).\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.homoglyphs = None\n","\n","    def __call__(self, homoglyphed_str: str) -> str:\n","        # find canon:\n","        target_category, all_categories = self._categorize_text(homoglyphed_str)\n","        homoglyph_table = self._select_canon_category_and_load(target_category, all_categories)\n","        return self._sanitize_text(target_category, homoglyph_table, homoglyphed_str)\n","\n","    def _categorize_text(self, text: str) -> dict:\n","        iso_categories = defaultdict(int)\n","        # self.iso_languages = defaultdict(int)\n","\n","        for char in text:\n","            iso_categories[Categories.detect(char)] += 1\n","            # for lang in hg.Languages.detect(char):\n","            #     self.iso_languages[lang] += 1\n","        target_category = max(iso_categories, key=iso_categories.get)\n","        all_categories = tuple(iso_categories)\n","        return target_category, all_categories\n","\n","    @cache\n","    def _select_canon_category_and_load(\n","        self, target_category: str, all_categories: tuple[str]\n","    ) -> dict:\n","        homoglyph_table = Homoglyphs(\n","            categories=(target_category, \"COMMON\")\n","        )  # alphabet loaded here from file\n","\n","        source_alphabet = Categories.get_alphabet(all_categories)\n","        restricted_table = homoglyph_table.get_restricted_table(\n","            source_alphabet, homoglyph_table.alphabet\n","        )  # table loaded here from file\n","        return restricted_table\n","\n","    def _sanitize_text(\n","        self, target_category: str, homoglyph_table: dict, homoglyphed_str: str\n","    ) -> str:\n","        sanitized_text = \"\"\n","        for char in homoglyphed_str:\n","            # langs = hg.Languages.detect(char)\n","            cat = Categories.detect(char)\n","            if target_category in cat or \"COMMON\" in cat or len(cat) == 0:\n","                sanitized_text += char\n","            else:\n","                sanitized_text += list(homoglyph_table[char])[0]\n","        return sanitized_text\n","\n","\n","class UnicodeSanitizer:\n","    \"\"\"Regex-based unicode sanitzer. Has different levels of granularity.\n","\n","    * ruleset=\"whitespaces\"    - attempts to remove only whitespace unicode characters\n","    * ruleset=\"IDN.blacklist\"  - does its best to remove unusual unicode based on  Network.IDN.blacklist characters\n","    * ruleset=\"ascii\"          - brute-forces all text into ascii\n","\n","    This is unlikely to be a comprehensive list.\n","\n","    You can find a more comprehensive discussion at https://www.unicode.org/reports/tr36/\n","    and https://www.unicode.org/faq/security.html\n","    \"\"\"\n","\n","    def __init__(self, ruleset=\"whitespaces\"):\n","        if ruleset == \"whitespaces\":\n","            \"\"\"Documentation:\n","            \\u00A0: Non-breaking space\n","            \\u1680: Ogham space mark\n","            \\u180E: Mongolian vowel separator\n","            \\u2000-\\u200B: Various space characters, including en space, em space, thin space, hair space, zero-width space, and zero-width non-joiner\n","            \\u200C\\u200D: Zero-width non-joiner and zero-width joiner\n","            \\u200E,\\u200F: Left-to-right-mark, Right-to-left-mark\n","            \\u2060: Word joiner\n","            \\u2063: Invisible separator\n","            \\u202F: Narrow non-breaking space\n","            \\u205F: Medium mathematical space\n","            \\u3000: Ideographic space\n","            \\uFEFF: Zero-width non-breaking space\n","            \\uFFA0: Halfwidth hangul filler\n","            \\uFFF9\\uFFFA\\uFFFB: Interlinear annotation characters\n","            \\uFE00-\\uFE0F: Variation selectors\n","            \\u202A-\\u202F: Embedding characters\n","            \\u3164: Korean hangul filler.\n","\n","            Note that these characters are not always superfluous whitespace characters!\n","            \"\"\"\n","\n","            self.pattern = re.compile(\n","                r\"[\\u00A0\\u1680\\u180E\\u2000-\\u200B\\u200C\\u200D\\u200E\\u200F\\u2060\\u2063\\u202F\\u205F\\u3000\\uFEFF\\uFFA0\\uFFF9\\uFFFA\\uFFFB\"\n","                r\"\\uFE00\\uFE01\\uFE02\\uFE03\\uFE04\\uFE05\\uFE06\\uFE07\\uFE08\\uFE09\\uFE0A\\uFE0B\\uFE0C\\uFE0D\\uFE0E\\uFE0F\\u3164\\u202A\\u202B\\u202C\\u202D\"\n","                r\"\\u202E\\u202F]\"\n","            )\n","        elif ruleset == \"IDN.blacklist\":\n","            \"\"\"Documentation:\n","            [\\u00A0\\u1680\\u180E\\u2000-\\u200B\\u202F\\u205F\\u2060\\u2063\\uFEFF]: Matches any whitespace characters in the Unicode character\n","                        set that are included in the IDN blacklist.\n","            \\uFFF9-\\uFFFB: Matches characters that are not defined in Unicode but are used as language tags in various legacy encodings.\n","                        These characters are not allowed in domain names.\n","            \\uD800-\\uDB7F: Matches the first part of a surrogate pair. Surrogate pairs are used to represent characters in the Unicode character\n","                        set that cannot be represented by a single 16-bit value. The first part of a surrogate pair is in the range U+D800 to U+DBFF,\n","                        and the second part is in the range U+DC00 to U+DFFF.\n","            \\uDB80-\\uDBFF][\\uDC00-\\uDFFF]?: Matches the second part of a surrogate pair. The second part of a surrogate pair is in the range U+DC00\n","                        to U+DFFF, and is optional.\n","            [\\uDB40\\uDC20-\\uDB40\\uDC7F][\\uDC00-\\uDFFF]: Matches certain invalid UTF-16 sequences which should not appear in IDNs.\n","            \"\"\"\n","\n","            self.pattern = re.compile(\n","                r\"[\\u00A0\\u1680\\u180E\\u2000-\\u200B\\u202F\\u205F\\u2060\\u2063\\uFEFF\\uFFF9-\\uFFFB\\uD800-\\uDB7F\\uDB80-\\uDBFF]\"\n","                r\"[\\uDC00-\\uDFFF]?|[\\uDB40\\uDC20-\\uDB40\\uDC7F][\\uDC00-\\uDFFF]\"\n","            )\n","        else:\n","            \"\"\"Documentation:\n","            This is a simple restriction to \"no-unicode\", using only ascii characters. Control characters are included.\n","            \"\"\"\n","            self.pattern = re.compile(r\"[^\\x00-\\x7F]+\")\n","\n","    def __call__(self, text: str) -> str:\n","        text = unicodedata.normalize(\"NFC\", text)  # canon forms\n","        text = self.pattern.sub(\" \", text)  # pattern match\n","        text = re.sub(\" +\", \" \", text)  # collapse whitespaces\n","        text = \"\".join(\n","            c for c in text if unicodedata.category(c) != \"Cc\"\n","        )  # Remove any remaining non-printable characters\n","        return text\n","\n","\n","class TrueCaser:\n","    \"\"\"True-casing, is a capitalization normalization that returns text to its original capitalization.\n","\n","    This defends against attacks that wRIte TeXt lIkE spOngBoB.\n","\n","    Here, a simple POS-tagger is used.\n","    \"\"\"\n","\n","    uppercase_pos = [\"PROPN\"]  # Name POS tags that should be upper-cased\n","\n","    def __init__(self, backend=\"spacy\"):\n","        if backend == \"spacy\":\n","            import spacy\n","\n","            self.nlp = spacy.load(\"en_core_web_sm\")\n","            self.normalize_fn = self._spacy_truecasing\n","        else:\n","            from nltk import pos_tag, word_tokenize  # noqa\n","            import nltk\n","\n","            nltk.download(\"punkt\")\n","            nltk.download(\"averaged_perceptron_tagger\")\n","            nltk.download(\"universal_tagset\")\n","            self.normalize_fn = self._nltk_truecasing\n","\n","    def __call__(self, random_capitalized_string: str) -> str:\n","        truecased_str = self.normalize_fn(random_capitalized_string)\n","        return truecased_str\n","\n","    def _spacy_truecasing(self, random_capitalized_string: str):\n","        doc = self.nlp(random_capitalized_string.lower())\n","        POS = self.uppercase_pos\n","        truecased_str = \"\".join(\n","            [\n","                w.text_with_ws.capitalize() if w.pos_ in POS or w.is_sent_start else w.text_with_ws\n","                for w in doc\n","            ]\n","        )\n","        return truecased_str\n","\n","    def _nltk_truecasing(self, random_capitalized_string: str):\n","        from nltk import pos_tag, word_tokenize\n","        import nltk\n","\n","        nltk.download(\"punkt\")\n","        nltk.download(\"averaged_perceptron_tagger\")\n","        nltk.download(\"universal_tagset\")\n","        POS = [\"NNP\", \"NNPS\"]\n","\n","        tagged_text = pos_tag(word_tokenize(random_capitalized_string.lower()))\n","        truecased_str = \" \".join([w.capitalize() if p in POS else w for (w, p) in tagged_text])\n","        return truecased_str\n"],"metadata":{"id":"ByRvoMD4BYA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######### EXTENDED WATERMARK PROCESSOR.py ##########\n","# coding=utf-8\n","# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n","# available at https://arxiv.org/abs/2301.10226\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from __future__ import annotations\n","import collections\n","from math import sqrt\n","from itertools import chain, tee\n","from functools import lru_cache\n","\n","import scipy.stats\n","#import torch\n","from tokenizers import Tokenizer\n","from transformers import LogitsProcessor\n","\n","# from normalizers import normalization_strategy_lookup\n","# from alternative_prf_schemes import prf_lookup, seeding_scheme_lookup\n","\n","\n","class WatermarkBase:\n","    def __init__(\n","        self,\n","        vocab: list[int] = None,\n","        gamma: float = 0.25,\n","        delta: float = 2.0,\n","        seeding_scheme: str = \"selfhash\",  # simple default, find more schemes in alternative_prf_schemes.py\n","        select_green_tokens: bool = True,  # should always be the default if not running in legacy mode\n","    ):\n","        # patch now that None could now maybe be passed as seeding_scheme\n","        if seeding_scheme is None:\n","            seeding_scheme = \"selfhash\"\n","\n","        # Vocabulary setup\n","        self.vocab = vocab\n","        self.vocab_size = len(vocab)\n","\n","        # Watermark behavior:\n","        self.gamma = gamma\n","        self.delta = delta\n","        self.rng = None\n","        self._initialize_seeding_scheme(seeding_scheme)\n","        # Legacy behavior:\n","        self.select_green_tokens = select_green_tokens\n","\n","    def _initialize_seeding_scheme(self, seeding_scheme: str) -> None:\n","        \"\"\"Initialize all internal settings of the seeding strategy from a colloquial, \"public\" name for the scheme.\"\"\"\n","        self.prf_type, self.context_width, self.self_salt, self.hash_key = seeding_scheme_lookup(seeding_scheme)\n","\n","    def _seed_rng(self, input_ids: torch.LongTensor) -> None:\n","        \"\"\"Seed RNG from local context. Not batched, because the generators we use (like cuda.random) are not batched.\"\"\"\n","        # Need to have enough context for seed generation\n","        if input_ids.shape[-1] < self.context_width:\n","            raise ValueError(f\"seeding_scheme requires at least a {self.context_width} token prefix to seed the RNG.\")\n","\n","        prf_key = prf_lookup[self.prf_type](input_ids[-self.context_width :], salt_key=self.hash_key)\n","        # enable for long, interesting streams of pseudorandom numbers: print(prf_key)\n","        self.rng.manual_seed(prf_key % (2**64 - 1))  # safeguard against overflow from long\n","\n","    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> torch.LongTensor:\n","        \"\"\"Seed rng based on local context width and use this information to generate ids on the green list.\"\"\"\n","        self._seed_rng(input_ids)\n","\n","        greenlist_size = int(self.vocab_size * self.gamma)\n","        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n","        if self.select_green_tokens:  # directly\n","            greenlist_ids = vocab_permutation[:greenlist_size]  # new\n","        else:  # select green via red\n","            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior\n","        return greenlist_ids\n","\n","\n","class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\n","    \"\"\"LogitsProcessor modifying model output scores in a pipe. Can be used in any HF pipeline to modify scores to fit the watermark,\n","    but can also be used as a standalone tool inserted for any model producing scores inbetween model outputs and next token sampler.\n","    \"\"\"\n","\n","    def __init__(self, *args, store_spike_ents: bool = False, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","        self.store_spike_ents = store_spike_ents\n","        self.spike_entropies = None\n","        if self.store_spike_ents:\n","            self._init_spike_entropies()\n","\n","    def _init_spike_entropies(self):\n","        alpha = torch.exp(torch.tensor(self.delta)).item()\n","        gamma = self.gamma\n","\n","        self.z_value = ((1 - gamma) * (alpha - 1)) / (1 - gamma + (alpha * gamma))\n","        self.expected_gl_coef = (gamma * alpha) / (1 - gamma + (alpha * gamma))\n","\n","        # catch for overflow when bias is \"infinite\"\n","        if alpha == torch.inf:\n","            self.z_value = 1.0\n","            self.expected_gl_coef = 1.0\n","\n","    def _get_spike_entropies(self):\n","        spike_ents = [[] for _ in range(len(self.spike_entropies))]\n","        for b_idx, ent_tensor_list in enumerate(self.spike_entropies):\n","            for ent_tensor in ent_tensor_list:\n","                spike_ents[b_idx].append(ent_tensor.item())\n","        return spike_ents\n","\n","    def _get_and_clear_stored_spike_ents(self):\n","        spike_ents = self._get_spike_entropies()\n","        self.spike_entropies = None\n","        return spike_ents\n","\n","    def _compute_spike_entropy(self, scores):\n","        # precomputed z value in init\n","        probs = scores.softmax(dim=-1)\n","        denoms = 1 + (self.z_value * probs)\n","        renormed_probs = probs / denoms\n","        sum_renormed_probs = renormed_probs.sum()\n","        return sum_renormed_probs\n","\n","    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n","        # Cannot lose loop, greenlists might have different lengths\n","        green_tokens_mask = torch.zeros_like(scores, dtype=torch.bool)\n","        for b_idx, greenlist in enumerate(greenlist_token_ids):\n","            if len(greenlist) > 0:\n","                green_tokens_mask[b_idx][greenlist] = True\n","        return green_tokens_mask\n","\n","    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:\n","        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n","        return scores\n","\n","    def _score_rejection_sampling(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, tail_rule=\"fixed_compute\") -> list[int]:\n","        \"\"\"Generate greenlist based on current candidate next token. Reject and move on if necessary. Method not batched.\n","        This is only a partial version of Alg.3 \"Robust Private Watermarking\", as it always assumes greedy sampling. It will still (kinda)\n","        work for all types of sampling, but less effectively.\n","        To work efficiently, this function can switch between a number of rules for handling the distribution tail.\n","        These are not exposed by default.\n","        \"\"\"\n","        sorted_scores, greedy_predictions = scores.sort(dim=-1, descending=True)\n","\n","        final_greenlist = []\n","        for idx, prediction_candidate in enumerate(greedy_predictions):\n","            greenlist_ids = self._get_greenlist_ids(torch.cat([input_ids, prediction_candidate[None]], dim=0))  # add candidate to prefix\n","            if prediction_candidate in greenlist_ids:  # test for consistency\n","                final_greenlist.append(prediction_candidate)\n","\n","            # What follows below are optional early-stopping rules for efficiency\n","            if tail_rule == \"fixed_score\":\n","                if sorted_scores[0] - sorted_scores[idx + 1] > self.delta:\n","                    break\n","            elif tail_rule == \"fixed_list_length\":\n","                if len(final_greenlist) == 10:\n","                    break\n","            elif tail_rule == \"fixed_compute\":\n","                if idx == 40:\n","                    break\n","            else:\n","                pass  # do not break early\n","        return torch.as_tensor(final_greenlist, device=input_ids.device)\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n","        \"\"\"Call with previous context as input_ids, and scores for next token.\"\"\"\n","\n","        # this is lazy to allow us to co-locate on the watermarked model's device\n","        self.rng = torch.Generator(device=input_ids.device) if self.rng is None else self.rng\n","\n","        # NOTE, it would be nice to get rid of this batch loop, but currently,\n","        # the seed and partition operations are not tensor/vectorized, thus\n","        # each sequence in the batch needs to be treated separately.\n","\n","        list_of_greenlist_ids = [None for _ in input_ids]  # Greenlists could differ in length\n","        for b_idx, input_seq in enumerate(input_ids):\n","            if self.self_salt:\n","                greenlist_ids = self._score_rejection_sampling(input_seq, scores[b_idx])\n","            else:\n","                greenlist_ids = self._get_greenlist_ids(input_seq)\n","            list_of_greenlist_ids[b_idx] = greenlist_ids\n","\n","            # logic for computing and storing spike entropies for analysis\n","            if self.store_spike_ents:\n","                if self.spike_entropies is None:\n","                    self.spike_entropies = [[] for _ in range(input_ids.shape[0])]\n","                self.spike_entropies[b_idx].append(self._compute_spike_entropy(scores[b_idx]))\n","\n","        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=list_of_greenlist_ids)\n","        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)\n","\n","        return scores\n","\n","\n","class WatermarkDetector(WatermarkBase):\n","    \"\"\"This is the detector for all watermarks imprinted with WatermarkLogitsProcessor.\n","\n","    The detector needs to be given the exact same settings that were given during text generation  to replicate the watermark\n","    greenlist generation and so detect the watermark.\n","    This includes the correct device that was used during text generation, the correct tokenizer, the correct\n","    seeding_scheme name, and parameters (delta, gamma).\n","\n","    Optional arguments are\n","    * normalizers [\"unicode\", \"homoglyphs\", \"truecase\"] -> These can mitigate modifications to generated text that could trip the watermark\n","    * ignore_repeated_ngrams -> This option changes the detection rules to count every unique ngram only once.\n","    * z_threshold -> Changing this threshold will change the sensitivity of the detector.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        *args,\n","        device: torch.device = None,\n","        tokenizer: Tokenizer = None,\n","        z_threshold: float = 4.0,\n","        normalizers: list[str] = [\"unicode\"],  # or also: [\"unicode\", \"homoglyphs\", \"truecase\"]\n","        ignore_repeated_ngrams: bool = True,\n","        **kwargs,\n","    ):\n","        super().__init__(*args, **kwargs)\n","        # also configure the metrics returned/preprocessing options\n","        assert device, \"Must pass device\"\n","        assert tokenizer, \"Need an instance of the generating tokenizer to perform detection\"\n","\n","        self.tokenizer = tokenizer\n","        self.device = device\n","        self.z_threshold = z_threshold\n","        self.rng = torch.Generator(device=self.device)\n","\n","        self.normalizers = []\n","        for normalization_strategy in normalizers:\n","            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))\n","        self.ignore_repeated_ngrams = ignore_repeated_ngrams\n","\n","    def dummy_detect(\n","        self,\n","        return_prediction: bool = True,\n","        return_scores: bool = True,\n","        z_threshold: float = None,\n","        return_num_tokens_scored: bool = True,\n","        return_num_green_tokens: bool = True,\n","        return_green_fraction: bool = True,\n","        return_green_token_mask: bool = False,\n","        return_all_window_scores: bool = False,\n","        return_z_score: bool = True,\n","        return_z_at_T: bool = True,\n","        return_p_value: bool = True,\n","    ):\n","        # HF-style output dictionary\n","        score_dict = dict()\n","        if return_num_tokens_scored:\n","            score_dict.update(dict(num_tokens_scored=float(\"nan\")))\n","        if return_num_green_tokens:\n","            score_dict.update(dict(num_green_tokens=float(\"nan\")))\n","        if return_green_fraction:\n","            score_dict.update(dict(green_fraction=float(\"nan\")))\n","        if return_z_score:\n","            score_dict.update(dict(z_score=float(\"nan\")))\n","        if return_p_value:\n","            z_score = score_dict.get(\"z_score\")\n","            if z_score is None:\n","                z_score = float(\"nan\")\n","            score_dict.update(dict(p_value=float(\"nan\")))\n","        if return_green_token_mask:\n","            score_dict.update(dict(green_token_mask=[]))\n","        if return_all_window_scores:\n","            score_dict.update(dict(window_list=[]))\n","        if return_z_at_T:\n","            score_dict.update(dict(z_score_at_T=torch.tensor([])))\n","\n","        output_dict = {}\n","        if return_scores:\n","            output_dict.update(score_dict)\n","        # if passed return_prediction then perform the hypothesis test and return the outcome\n","        if return_prediction:\n","            z_threshold = z_threshold if z_threshold else self.z_threshold\n","            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\n","            output_dict[\"prediction\"] = False\n","\n","        return output_dict\n","\n","    def _compute_z_score(self, observed_count, T):\n","        # count refers to number of green tokens, T is total number of tokens\n","        expected_count = self.gamma\n","        numer = observed_count - expected_count * T\n","        denom = sqrt(T * expected_count * (1 - expected_count))\n","        z = numer / denom\n","        return z\n","\n","    def _compute_p_value(self, z):\n","        p_value = scipy.stats.norm.sf(z)\n","        return p_value\n","\n","    @lru_cache(maxsize=2**32)\n","    def _get_ngram_score_cached(self, prefix: tuple[int], target: int):\n","        \"\"\"Expensive re-seeding and sampling is cached.\"\"\"\n","        # Handle with care, should ideally reset on __getattribute__ access to self.prf_type, self.context_width, self.self_salt, self.hash_key\n","        greenlist_ids = self._get_greenlist_ids(torch.as_tensor(prefix, device=self.device))\n","        return True if target in greenlist_ids else False\n","\n","    def _score_ngrams_in_passage(self, input_ids: torch.Tensor):\n","        \"\"\"Core function to gather all ngrams in the input and compute their watermark.\"\"\"\n","        if len(input_ids) - self.context_width < 1:\n","            raise ValueError(\n","                f\"Must have at least {1} token to score after \"\n","                f\"the first min_prefix_len={self.context_width} tokens required by the seeding scheme.\"\n","            )\n","\n","        # Compute scores for all ngrams contexts in the passage:\n","        token_ngram_generator = ngrams(input_ids.cpu().tolist(), self.context_width + 1 - self.self_salt)\n","        frequencies_table = collections.Counter(token_ngram_generator)\n","        ngram_to_watermark_lookup = {}\n","        for idx, ngram_example in enumerate(frequencies_table.keys()):\n","            prefix = ngram_example if self.self_salt else ngram_example[:-1]\n","            target = ngram_example[-1]\n","            ngram_to_watermark_lookup[ngram_example] = self._get_ngram_score_cached(prefix, target)\n","\n","        return ngram_to_watermark_lookup, frequencies_table\n","\n","    def _get_green_at_T_booleans(self, input_ids, ngram_to_watermark_lookup) -> tuple[torch.Tensor]:\n","        \"\"\"Generate binary list of green vs. red per token, a separate list that ignores repeated ngrams, and a list of offsets to\n","        convert between both representations:\n","        green_token_mask = green_token_mask_unique[offsets] except for all locations where otherwise a repeat would be counted\n","        \"\"\"\n","        green_token_mask, green_token_mask_unique, offsets = [], [], []\n","        used_ngrams = {}\n","        unique_ngram_idx = 0\n","        ngram_examples = ngrams(input_ids.cpu().tolist(), self.context_width + 1 - self.self_salt)\n","\n","        for idx, ngram_example in enumerate(ngram_examples):\n","            green_token_mask.append(ngram_to_watermark_lookup[ngram_example])\n","            if self.ignore_repeated_ngrams:\n","                if ngram_example in used_ngrams:\n","                    pass\n","                else:\n","                    used_ngrams[ngram_example] = True\n","                    unique_ngram_idx += 1\n","                    green_token_mask_unique.append(ngram_to_watermark_lookup[ngram_example])\n","            else:\n","                green_token_mask_unique.append(ngram_to_watermark_lookup[ngram_example])\n","                unique_ngram_idx += 1\n","            offsets.append(unique_ngram_idx - 1)\n","        return (\n","            torch.tensor(green_token_mask),\n","            torch.tensor(green_token_mask_unique),\n","            torch.tensor(offsets),\n","        )\n","\n","    def _score_sequence(\n","        self,\n","        input_ids: torch.Tensor,\n","        return_num_tokens_scored: bool = True,\n","        return_num_green_tokens: bool = True,\n","        return_green_fraction: bool = True,\n","        return_green_token_mask: bool = False,\n","        return_z_score: bool = True,\n","        return_z_at_T: bool = True,\n","        return_p_value: bool = True,\n","    ):\n","        ngram_to_watermark_lookup, frequencies_table = self._score_ngrams_in_passage(input_ids)\n","        green_token_mask, green_unique, offsets = self._get_green_at_T_booleans(input_ids, ngram_to_watermark_lookup)\n","\n","        # Count up scores over all ngrams\n","        if self.ignore_repeated_ngrams:\n","            # Method that only counts a green/red hit once per unique ngram.\n","            # New num total tokens scored (T) becomes the number unique ngrams.\n","            # We iterate over all unqiue token ngrams in the input, computing the greenlist\n","            # induced by the context in each, and then checking whether the last\n","            # token falls in that greenlist.\n","            num_tokens_scored = len(frequencies_table.keys())\n","            green_token_count = sum(ngram_to_watermark_lookup.values())\n","        else:\n","            num_tokens_scored = sum(frequencies_table.values())\n","            assert num_tokens_scored == len(input_ids) - self.context_width + self.self_salt\n","            green_token_count = sum(freq * outcome for freq, outcome in zip(frequencies_table.values(), ngram_to_watermark_lookup.values()))\n","        assert green_token_count == green_unique.sum()\n","\n","        # HF-style output dictionary\n","        score_dict = dict()\n","        if return_num_tokens_scored:\n","            score_dict.update(dict(num_tokens_scored=num_tokens_scored))\n","        if return_num_green_tokens:\n","            score_dict.update(dict(num_green_tokens=green_token_count))\n","        if return_green_fraction:\n","            score_dict.update(dict(green_fraction=(green_token_count / num_tokens_scored)))\n","        if return_z_score:\n","            score_dict.update(dict(z_score=self._compute_z_score(green_token_count, num_tokens_scored)))\n","        if return_p_value:\n","            z_score = score_dict.get(\"z_score\")\n","            if z_score is None:\n","                z_score = self._compute_z_score(green_token_count, num_tokens_scored)\n","            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\n","        if return_green_token_mask:\n","            score_dict.update(dict(green_token_mask=green_token_mask.tolist()))\n","        if return_z_at_T:\n","            # Score z_at_T separately:\n","            sizes = torch.arange(1, len(green_unique) + 1)\n","            seq_z_score_enum = torch.cumsum(green_unique, dim=0) - self.gamma * sizes\n","            seq_z_score_denom = torch.sqrt(sizes * self.gamma * (1 - self.gamma))\n","            z_score_at_effective_T = seq_z_score_enum / seq_z_score_denom\n","            z_score_at_T = z_score_at_effective_T[offsets]\n","            assert torch.isclose(z_score_at_T[-1], torch.tensor(z_score))\n","\n","            score_dict.update(dict(z_score_at_T=z_score_at_T))\n","\n","        return score_dict\n","\n","    def _score_windows_impl_batched(\n","        self,\n","        input_ids: torch.Tensor,\n","        window_size: str,\n","        window_stride: int = 1,\n","    ):\n","        # Implementation details:\n","        # 1) --ignore_repeated_ngrams is applied globally, and windowing is then applied over the reduced binary vector\n","        #      this is only one way of doing it, another would be to ignore bigrams within each window (maybe harder to parallelize that)\n","        # 2) These windows on the binary vector of green/red hits, independent of context_width, in contrast to Kezhi's first implementation\n","        # 3) z-scores from this implementation cannot be directly converted to p-values, and should only be used as labels for a\n","        #    ROC chart that calibrates to a chosen FPR. Due, to windowing, the multiple hypotheses will increase scores across the board#\n","        #    naive_count_correction=True is a partial remedy to this\n","\n","        ngram_to_watermark_lookup, frequencies_table = self._score_ngrams_in_passage(input_ids)\n","        green_mask, green_ids, offsets = self._get_green_at_T_booleans(input_ids, ngram_to_watermark_lookup)\n","        len_full_context = len(green_ids)\n","\n","        partial_sum_id_table = torch.cumsum(green_ids, dim=0)\n","\n","        if window_size == \"max\":\n","            # could start later, small window sizes cannot generate enough power\n","            # more principled: solve (T * Spike_Entropy - g * T) / sqrt(T * g * (1 - g)) = z_thresh for T\n","            sizes = range(1, len_full_context)\n","        else:\n","            sizes = [int(x) for x in window_size.split(\",\") if len(x) > 0]\n","\n","        z_score_max_per_window = torch.zeros(len(sizes))\n","        cumulative_eff_z_score = torch.zeros(len_full_context)\n","        s = window_stride\n","\n","        window_fits = False\n","        for idx, size in enumerate(sizes):\n","            if size <= len_full_context:\n","                # Compute hits within window for all positions in parallel:\n","                window_score = torch.zeros(len_full_context - size + 1, dtype=torch.long)\n","                # Include 0-th window\n","                window_score[0] = partial_sum_id_table[size - 1]\n","                # All other windows from the 1st:\n","                window_score[1:] = partial_sum_id_table[size::s] - partial_sum_id_table[:-size:s]\n","\n","                # Now compute batched z_scores\n","                batched_z_score_enum = window_score - self.gamma * size\n","                z_score_denom = sqrt(size * self.gamma * (1 - self.gamma))\n","                batched_z_score = batched_z_score_enum / z_score_denom\n","\n","                # And find the maximal hit\n","                maximal_z_score = batched_z_score.max()\n","                z_score_max_per_window[idx] = maximal_z_score\n","\n","                z_score_at_effective_T = torch.cummax(batched_z_score, dim=0)[0]\n","                cumulative_eff_z_score[size::s] = torch.maximum(cumulative_eff_z_score[size::s], z_score_at_effective_T[:-1])\n","                window_fits = True  # successful computation for any window in sizes\n","\n","        if not window_fits:\n","            raise ValueError(\n","                f\"Could not find a fitting window with window sizes {window_size} for (effective) context length {len_full_context}.\"\n","            )\n","\n","        # Compute optimal window size and z-score\n","        cumulative_z_score = cumulative_eff_z_score[offsets]\n","        optimal_z, optimal_window_size_idx = z_score_max_per_window.max(dim=0)\n","        optimal_window_size = sizes[optimal_window_size_idx]\n","        return (\n","            optimal_z,\n","            optimal_window_size,\n","            z_score_max_per_window,\n","            cumulative_z_score,\n","            green_mask,\n","        )\n","\n","    def _score_sequence_window(\n","        self,\n","        input_ids: torch.Tensor,\n","        return_num_tokens_scored: bool = True,\n","        return_num_green_tokens: bool = True,\n","        return_green_fraction: bool = True,\n","        return_green_token_mask: bool = False,\n","        return_z_score: bool = True,\n","        return_z_at_T: bool = True,\n","        return_p_value: bool = True,\n","        window_size: str = None,\n","        window_stride: int = 1,\n","    ):\n","        (\n","            optimal_z,\n","            optimal_window_size,\n","            _,\n","            z_score_at_T,\n","            green_mask,\n","        ) = self._score_windows_impl_batched(input_ids, window_size, window_stride)\n","\n","        # HF-style output dictionary\n","        score_dict = dict()\n","        if return_num_tokens_scored:\n","            score_dict.update(dict(num_tokens_scored=optimal_window_size))\n","\n","        denom = sqrt(optimal_window_size * self.gamma * (1 - self.gamma))\n","        green_token_count = int(optimal_z * denom + self.gamma * optimal_window_size)\n","        green_fraction = green_token_count / optimal_window_size\n","        if return_num_green_tokens:\n","            score_dict.update(dict(num_green_tokens=green_token_count))\n","        if return_green_fraction:\n","            score_dict.update(dict(green_fraction=green_fraction))\n","        if return_z_score:\n","            score_dict.update(dict(z_score=optimal_z))\n","        if return_z_at_T:\n","            score_dict.update(dict(z_score_at_T=z_score_at_T))\n","        if return_p_value:\n","            z_score = score_dict.get(\"z_score\", optimal_z)\n","            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\n","\n","        # Return per-token results for mask. This is still the same, just scored by windows\n","        # todo would be to mark the actually counted tokens differently\n","        if return_green_token_mask:\n","            score_dict.update(dict(green_token_mask=green_mask.tolist()))\n","\n","        return score_dict\n","\n","    def detect(\n","        self,\n","        text: str = None,\n","        tokenized_text: list[int] = None,\n","        window_size: str = None,\n","        window_stride: int = None,\n","        return_prediction: bool = True,\n","        return_scores: bool = True,\n","        z_threshold: float = None,\n","        convert_to_float: bool = False,\n","        **kwargs,\n","    ) -> dict:\n","        \"\"\"Scores a given string of text and returns a dictionary of results.\"\"\"\n","\n","        assert (text is not None) ^ (tokenized_text is not None), \"Must pass either the raw or tokenized string\"\n","        if return_prediction:\n","            kwargs[\"return_p_value\"] = True  # to return the \"confidence\":=1-p of positive detections\n","\n","        # run optional normalizers on text\n","        for normalizer in self.normalizers:\n","            text = normalizer(text)\n","        if len(self.normalizers) > 0:\n","            print(f\"Text after normalization:\\n\\n{text}\\n\")\n","\n","        if tokenized_text is None:\n","            assert self.tokenizer is not None, (\n","                \"Watermark detection on raw string \",\n","                \"requires an instance of the tokenizer \",\n","                \"that was used at generation time.\",\n","            )\n","            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(self.device)\n","            if tokenized_text[0] == self.tokenizer.bos_token_id:\n","                tokenized_text = tokenized_text[1:]\n","        else:\n","            # try to remove the bos_tok at beginning if it's there\n","            if (self.tokenizer is not None) and (tokenized_text[0] == self.tokenizer.bos_token_id):\n","                tokenized_text = tokenized_text[1:]\n","\n","        # call score method\n","        output_dict = {}\n","\n","        if window_size is not None:\n","            # assert window_size <= len(tokenized_text) cannot assert for all new types\n","            score_dict = self._score_sequence_window(\n","                tokenized_text,\n","                window_size=window_size,\n","                window_stride=window_stride,\n","                **kwargs,\n","            )\n","            output_dict.update(score_dict)\n","        else:\n","            score_dict = self._score_sequence(tokenized_text, **kwargs)\n","        if return_scores:\n","            output_dict.update(score_dict)\n","        # if passed return_prediction then perform the hypothesis test and return the outcome\n","        if return_prediction:\n","            z_threshold = z_threshold if z_threshold else self.z_threshold\n","            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\n","            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n","            if output_dict[\"prediction\"]:\n","                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n","\n","        # convert any numerical values to float if requested\n","        if convert_to_float:\n","            for key, value in output_dict.items():\n","                if isinstance(value, int):\n","                    output_dict[key] = float(value)\n","\n","        return output_dict\n","\n","\n","##########################################################################\n","# Ngram iteration from nltk, extracted to remove the dependency\n","# Natural Language Toolkit: Utility functions\n","#\n","# Copyright (C) 2001-2023 NLTK Project\n","# Author: Steven Bird <stevenbird1@gmail.com>\n","#         Eric Kafe <kafe.eric@gmail.com> (acyclic closures)\n","# URL: <https://www.nltk.org/>\n","# For license information, see https://github.com/nltk/nltk/blob/develop/LICENSE.txt\n","##########################################################################\n","\n","\n","def ngrams(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n","    sequence = iter(sequence)\n","    if pad_left:\n","        sequence = chain((pad_symbol,) * (n - 1), sequence)\n","    if pad_right:\n","        sequence = chain(sequence, (pad_symbol,) * (n - 1))\n","    iterables = tee(sequence, n)\n","\n","    for i, sub_iterable in enumerate(iterables):  # For each window,\n","        for _ in range(i):  # iterate through every order of ngrams\n","            next(sub_iterable, None)  # generate the ngrams within the window.\n","    return zip(*iterables)  # Unpack and flattens the iterables.\n"],"metadata":{"id":"5oWZASHID8a-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Testing"],"metadata":{"id":"r851jgdSjXe4"}},{"cell_type":"code","source":["#Load Dataset\n","wiki = pd.read_csv('GPT-wiki-intro.csv')S"],"metadata":{"id":"SQPsGPrWeaOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Setup for Easymark\n","def add_watermark(text): return text.replace(chr(0x0020), chr(0x2004))\n","def detect_watermark(sentence): return chr(0x2004) in sentence"],"metadata":{"id":"tP7JnpBcLwm-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Setup for Kirchenbauer's Watermark\n","#We need to define a model here, can just use one from huggingface\n","model_id = \"facebook/opt-1.3b\"\n","#model_id = \"bert-base-cased\"\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","device = torch.device(\"cuda\")\n","model = model.to(device)\n","model.eval()\n","\n","#Setup for Kirchenbauer's Detector\n","watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n","                                        gamma=0.25, # should match original setting\n","                                        seeding_scheme=\"selfhash\", # should match original setting\n","                                        device=model.device, # must match the original rng device type\n","                                        tokenizer=tokenizer,\n","                                        z_threshold=4.0,\n","                                        normalizers=[],\n","                                        ignore_repeated_ngrams=True)\n","#score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze"],"metadata":{"id":"GwEVGJHGkC-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def watermark_easy(data):\n","  watermarked = []\n","  for text in data:\n","    watermarked.append(add_watermark(text))\n","  return watermarked\n","\n","def detect_easy(data):\n","  results = []\n","  for watermarked in data:\n","    results.append(detect_watermark(watermarked))\n","  return results\n","\n","def watermark_kirch(data,tokenizer,model):\n","  watermarked = []\n","  for input_text in data:\n","    # tokenized_input = tokenizer(input_text).to(model.device)\n","    tokenized_input = tokenizer(input_text[:40], return_tensors=\"pt\")\n","    tokenized_input = {key: value.to(model.device) for key, value in tokenized_input.items()}\n","    # note that if the model is on cuda, then the input is on cuda\n","    # and thus the watermarking rng is cuda-based.\n","    # This is a different generator than the cpu-based rng in pytorch!\n","    watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n","                            gamma=0.25,\n","                            delta=2.0,\n","                            seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n","    output_tokens = model.generate(**tokenized_input,\n","                      logits_processor=LogitsProcessorList([watermark_processor]),\n","                      max_new_tokens = 50)\n","    output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n","    watermarked.append(output_text)\n","  return watermarked\n","\n","def detect_kirch(data,watermark_detector):\n","  results = []\n","  for watermarked in data:\n","    score_dict = watermark_detector.detect(watermarked)\n","    results.append(score_dict['prediction'])\n","  return results\n"],"metadata":{"id":"MXMu7SRDgSJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["baseline_easy = detect_easy(watermark_easy(wiki['generated_intro'][:100000]))\n","print(sum(baseline_easy)/len(baseline_easy))\n","print(sum(baseline_easy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR5As55BiDZW","executionInfo":{"status":"ok","timestamp":1702159178711,"user_tz":300,"elapsed":455,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"a20a0492-793a-4fe5-ece1-a64aca471b1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n","100000\n"]}]},{"cell_type":"code","source":["baseline_kirch = detect_kirch(watermark_kirch(wiki['generated_intro'][:10],tokenizer,model),watermark_detector)\n","print(sum(baseline_kirch)/len(baseline_kirch))\n","print(sum(baseline_kirch))"],"metadata":{"id":"Ng_2EOyinmtX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702159382711,"user_tz":300,"elapsed":96658,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"dcd5d419-3e91-423d-862c-52fc59afe210"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9\n","9\n"]}]},{"cell_type":"markdown","source":["# Original Code Below (not organized)"],"metadata":{"id":"1tIq5I1lrkkz"}},{"cell_type":"markdown","source":["# Watermarking"],"metadata":{"id":"o8_P-rxGLers"}},{"cell_type":"markdown","source":["**Watermark Text Kirchenbauer**"],"metadata":{"id":"KjU7ok77Q1iP"}},{"cell_type":"code","source":["# from gpt-wiki-into\n","# we will have to create the dataset, but i havent done that yet, just wanted to see results for one text\n","#input_text = \"Sexhow railway station was a railway station\"\n","input_text = wiki['generated_intro'][5]"],"metadata":{"id":"OGdZ-UAwZk5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_text[:50])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYbyuCXKmNfR","executionInfo":{"status":"ok","timestamp":1702159254493,"user_tz":300,"elapsed":120,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"df64f048-aa02-4926-f5f7-117771bc5f52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Moluccans are the Austronesian-speaking and Papuan\n"]}]},{"cell_type":"code","source":["# tokenized_input = tokenizer(input_text).to(model.device)\n","tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n","tokenized_input = {key: value.to(model.device) for key, value in tokenized_input.items()}\n","# note that if the model is on cuda, then the input is on cuda\n","# and thus the watermarking rng is cuda-based.\n","# This is a different generator than the cpu-based rng in pytorch!\n"],"metadata":{"id":"bgTPAL4acS92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_input"],"metadata":{"id":"4903wzXgt1sa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702159206295,"user_tz":300,"elapsed":105,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"9e724b41-d370-4b55-b352-cddae91261fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2, 35581,  9178, 10780,  1992,    21,    10, 10780,  1992]],\n","        device='cuda:0'),\n"," 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n","                                               gamma=0.25,\n","                                               delta=2.0,\n","                                               seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n","# Note:\n","# You can turn off self-hashing by setting the seeding scheme to `minhash`.\n","\n","#tokenized_input = tokenizer(input_text).to(model.device)\n","# note that if the model is on cuda, then the input is on cuda\n","# and thus the watermarking rng is cuda-based.\n","# This is a different generator than the cpu-based rng in pytorch!\n","\n","output_tokens = model.generate(**tokenized_input,\n","                              logits_processor=LogitsProcessorList([watermark_processor]),\n","                              max_new_tokens = 100)\n","#output_tokens = model.generate(**tokenized_input)\n","\n","# if decoder only model, then we need to isolate the\n","# newly generated tokens as only those are watermarked, the input/prompt is not\n","\n","#COMMENTED THIS LINE:\n","#output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n","\n","output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]"],"metadata":{"id":"n5FAYhFxTLSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_tokens"],"metadata":{"id":"fhtTtW_RAz1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"BpE12wTfa5eE","executionInfo":{"status":"ok","timestamp":1702159102979,"user_tz":300,"elapsed":107,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"7da4e799-ebd3-45c2-de11-4d3c552ab5bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Sexhow railway station was a railway station in the city of Bombay, India. It was situated on the railway line from the city of Bombay to the port of Bombay. The station was opened in 1857 by the Bombay Central Railway and closed in 1872.\\n\\nHistory\\n\\nThe railway line from Bombay to the port of Bombay was opened by the Bombay Central Railway on 18 March 1857. The station was opened by the Western Railway on 1 March 1859 and the East Indian Company on 1 March 1860. The station was situated on'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["input_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YTMKy2LU9MXn","executionInfo":{"status":"ok","timestamp":1702159107544,"user_tz":300,"elapsed":116,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"a3d7d01b-66b7-4ae5-f23a-6883e60590c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Sexhow railway station was a railway station'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["**Watermark Easymark**"],"metadata":{"id":"XsWXsdL4Ln1k"}},{"cell_type":"markdown","source":["# Watermark Detection"],"metadata":{"id":"ssV6sB4ZK7PK"}},{"cell_type":"markdown","source":["**Watermark Detection Kirchenbauer**\n","\n","\n"],"metadata":{"id":"IV8OWKR_QyMZ"}},{"cell_type":"code","source":["#Runs watermark detector\n","\n","watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n","                                        gamma=0.25, # should match original setting\n","                                        seeding_scheme=\"selfhash\", # should match original setting\n","                                        device=model.device, # must match the original rng device type\n","                                        tokenizer=tokenizer,\n","                                        z_threshold=4.0,\n","                                        normalizers=[],\n","                                        ignore_repeated_ngrams=True)\n","\n","#score_dict = watermark_detector.detect(watermarked_text_test) # or any other text of interest to analyze\n","score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze"],"metadata":{"id":"M7QkF6NoTPLe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhdfGh_VgTqK","executionInfo":{"status":"ok","timestamp":1702159111482,"user_tz":300,"elapsed":125,"user":{"displayName":"David Vapnek","userId":"08655883607384449045"}},"outputId":"d4c21ab0-b401-44fa-cdf7-ac91361fb1de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'num_tokens_scored': 95,\n"," 'num_green_tokens': 46,\n"," 'green_fraction': 0.4842105263157895,\n"," 'z_score': 5.271905012055922,\n"," 'p_value': 6.750747719037549e-08,\n"," 'z_score_at_T': tensor([-0.5774, -0.8165, -1.0000, -1.1547, -1.2910, -0.4714, -0.6547,  0.0000,\n","         -0.1925,  0.3651,  0.1741,  0.0000, -0.1601,  0.3086,  0.7454,  1.1547,\n","          0.9802,  0.8165,  1.1921,  1.0328,  1.3859,  1.7233,  2.0466,  1.8856,\n","          1.8856,  1.7321,  2.0381,  1.8889,  2.1822,  2.4659,  2.3190,  2.5924,\n","          2.4495,  2.3116,  2.5744,  2.8301,  2.6943,  2.9424,  3.1844,  3.4207,\n","          3.2863,  3.1558,  3.3853,  3.6098,  3.8297,  3.7009,  3.9158,  4.1265,\n","          4.0000,  3.8765,  3.7559,  3.6380,  3.5228,  3.4101,  3.2998,  3.5032,\n","          3.3947,  3.2883,  3.4873,  3.6831,  3.8759,  3.8759,  3.8759,  3.8759,\n","          3.7700,  3.6662,  3.8552,  4.0415,  3.9386,  3.9386,  3.9386,  4.1219,\n","          4.3026,  4.4809,  4.3788,  4.2784,  4.1797,  4.3546,  4.2571,  4.2571,\n","          4.2571,  4.4296,  4.4296,  4.6000,  4.5034,  4.4083,  4.5760,  4.7419,\n","          4.6476,  4.8113,  4.9731,  5.1332,  5.2915,  5.4482,  5.6032,  5.5090,\n","          5.4160,  5.5691,  5.4772,  5.3865,  5.2970,  5.2086,  5.2086,  5.3594,\n","          5.2719]),\n"," 'prediction': True,\n"," 'confidence': 0.9999999324925228}"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":[],"metadata":{"id":"ejmcUtyM4GA3"},"execution_count":null,"outputs":[]}]}